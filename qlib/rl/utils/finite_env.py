# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

"""
This is to support finite env in vector env.
See https://github.com/thu-ml/tianshou/issues/322 for details.
"""

from __future__ import annotations

import copy
import warnings
from contextlib import contextmanager
from typing import Any, Callable, Dict, Generator, List, Optional, Set, Tuple, Type, Union, cast

import gym
import numpy as np
from tianshou.env import BaseVectorEnv, DummyVectorEnv, ShmemVectorEnv, SubprocVectorEnv

from qlib.typehint import Literal

from .log import LogWriter

__all__ = [
    "generate_nan_observation",
    "check_nan_observation",
    "FiniteVectorEnv",
    "FiniteDummyVectorEnv",
    "FiniteSubprocVectorEnv",
    "FiniteShmemVectorEnv",
    "FiniteEnvType",
    "vectorize_env",
]

FiniteEnvType = Literal["dummy", "subproc", "shmem"]
T = Union[dict, list, tuple, np.ndarray]


def fill_invalid(obj: int | float | bool | T) -> T:
    if isinstance(obj, (int, float, bool)):
        return fill_invalid(np.array(obj))
    if hasattr(obj, "dtype"):
        if isinstance(obj, np.ndarray):
            if np.issubdtype(obj.dtype, np.floating):
                return np.full_like(obj, np.nan)
            return np.full_like(obj, np.iinfo(obj.dtype).max)
        # dealing with corner cases that numpy number is not supported by tianshou's sharray
        return fill_invalid(np.array(obj))
    elif isinstance(obj, dict):
        return {k: fill_invalid(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [fill_invalid(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(fill_invalid(v) for v in obj)
    raise ValueError(f"Unsupported value to fill with invalid: {obj}")


def is_invalid(arr: int | float | bool | T) -> bool:
    if isinstance(arr, np.ndarray):
        # handle floating arrays
        if np.issubdtype(arr.dtype, np.floating):
            return np.isnan(arr).all()
        # handle object-dtype arrays (e.g., arrays of dicts)
        if arr.dtype == object:
            try:
                return all(is_invalid(o) for o in arr.tolist())
            except Exception:
                return True
        # integer-like arrays
        return cast(bool, cast(np.ndarray, np.iinfo(arr.dtype).max == arr).all())
    if isinstance(arr, dict):
        return all(is_invalid(o) for o in arr.values())
    if isinstance(arr, (list, tuple)):
        return all(is_invalid(o) for o in arr)
    if isinstance(arr, (int, float, bool, np.number)):
        return is_invalid(np.array(arr))
    return True


def generate_nan_observation(obs_space: gym.Space) -> Any:
    """The NaN observation that indicates the environment receives no seed.

    We assume that obs is complex and there must be something like float.
    Otherwise this logic doesn't work.
    """

    sample = obs_space.sample()
    sample = fill_invalid(sample)
    return sample


def check_nan_observation(obs: Any) -> bool:
    """Check whether obs is generated by :func:`generate_nan_observation`."""
    return is_invalid(obs)


def _numeric_stack_or_pad(arrs: List[Any], target_len: int | None = None) -> np.ndarray:
    """Try to convert a list of observations into a numeric ndarray.

    Strategy:
    - If all elements are numpy arrays with identical shapes, use np.stack.
    - Else if elements are numeric arrays (ndarray) with varying shapes, flatten
      each to 1D and pad shorter ones with NaN to the maximum length.
    - Otherwise fall back to object-dtype array.
    """
    # If a target_len is supplied, use it. Otherwise infer from first numeric obs.
    numeric_arrays: List[Any] = []
    for a in arrs:
        try:
            na = np.asarray(a)
        except Exception:
            numeric_arrays.append(None)
            continue
        numeric_arrays.append(na)

    if target_len is None:
        # pick first non-object numeric array as target
        target = None
        for na in numeric_arrays:
            if isinstance(na, np.ndarray) and na.dtype != object:
                target = na
                break
        if target is None:
            return np.array(arrs, dtype=object)
        target_len = int(target.ravel().size)

    processed: List[np.ndarray] = []
    for a in arrs:
        try:
            na = np.asarray(a)
        except Exception:
            processed.append(np.array(a, dtype=object))
            continue
        if na.dtype == object:
            processed.append(np.array(a, dtype=object))
            continue
        flat = na.ravel()
        if flat.size == target_len:
            processed.append(flat.reshape((target_len,)))
        elif flat.size < target_len:
            padded = np.pad(flat, (0, target_len - flat.size), constant_values=np.nan)
            processed.append(padded)
        else:
            processed.append(flat[:target_len])

    return np.stack(processed)


class FiniteVectorEnv(BaseVectorEnv):
    """To allow the paralleled env workers consume a single DataQueue until it's exhausted.

    See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.

    The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case)
    consumed by exactly one environment. This is not possible by tianshou's native VectorEnv and Collector,
    because tianshou is unaware of this "exactly one" constraint, and might launch extra workers.

    Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue.
    The reset of two workers must be both called according to the logic in collect.
    The returned results of two workers are collected, regardless of what they are.
    The problem is, one of the reset result must be invalid, or repeated,
    because there's only one need in queue, and collector isn't aware of such situation.

    Luckily, we can hack the vector env, and make a protocol between single env and vector env.
    The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for
    reading from queue, and generate a special observation when the queue is exhausted. The special obs
    is called "nan observation", because simply using none causes problems in shared-memory vector env.
    :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan
    observation. It also maintains an ``_alive_env_ids`` to track which workers should never be
    called again. When also the environments are exhausted, it will raise StopIteration exception.

    The usage of this vector env in collector are two parts:

    1. If the data queue is finite (usually when inference), collector should collect "infinity" number of
       episodes, until the vector env exhausts by itself.
    2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.
       In this case, data would be randomly ordered, and some repetitions wouldn't matter.

    One extra function of this vector env is that it has a logger that explicitly collects logs
    from child workers. See :class:`qlib.rl.utils.LogWriter`.
    """

    _logger: list[LogWriter]

    def __init__(
        self, logger: LogWriter | list[LogWriter] | None, env_fns: list[Callable[..., gym.Env]], **kwargs: Any
    ) -> None:
        super().__init__(env_fns, **kwargs)

        if isinstance(logger, list):
            self._logger = logger
        elif isinstance(logger, LogWriter):
            self._logger = [logger]
        else:
            self._logger = []
        self._alive_env_ids: Set[int] = set()
        self._reset_alive_envs()
        self._default_obs = self._default_info = self._default_rew = None
        self._zombie = False

        self._collector_guarded: bool = False

    def _reset_alive_envs(self) -> None:
        if not self._alive_env_ids:
            # starting or running out
            self._alive_env_ids = set(range(self.env_num))

    # to workaround with tianshou's buffer and batch
    def _set_default_obs(self, obs: Any) -> None:
        if obs is not None and self._default_obs is None:
            self._default_obs = copy.deepcopy(obs)

    def _set_default_info(self, info: Any) -> None:
        if info is not None and self._default_info is None:
            self._default_info = copy.deepcopy(info)

    def _set_default_rew(self, rew: Any) -> None:
        if rew is not None and self._default_rew is None:
            self._default_rew = copy.deepcopy(rew)

    def _get_default_obs(self) -> Any:
        return copy.deepcopy(self._default_obs)

    def _get_default_info(self) -> Any:
        return copy.deepcopy(self._default_info)

    def _get_default_rew(self) -> Any:
        return copy.deepcopy(self._default_rew)

    # END

    @staticmethod
    def _postproc_env_obs(obs: Any) -> Optional[Any]:
        # reserved for shmem vector env to restore empty observation
        if obs is None or check_nan_observation(obs):
            return None
        return obs

    @contextmanager
    def collector_guard(self) -> Generator[FiniteVectorEnv, None, None]:
        """Guard the collector. Recommended to guard every collect.

        This guard is for two purposes.

        1. Catch and ignore the StopIteration exception, which is the stopping signal
           thrown by FiniteEnv to let tianshou know that ``collector.collect()`` should exit.
        2. Notify the loggers that the collect is ready / done what it's ready / done.

        Examples
        --------
        >>> with finite_env.collector_guard():
        ...     collector.collect(n_episode=INF)
        """
        self._collector_guarded = True

        for logger in self._logger:
            logger.on_env_all_ready()

        try:
            yield self
        except StopIteration:
            pass
        finally:
            self._collector_guarded = False

        # At last trigger the loggers
        for logger in self._logger:
            logger.on_env_all_done()

    def reset(
        self,
        id: int | List[int] | np.ndarray | None = None,
        env_id: int | List[int] | np.ndarray | None = None,
        **kwargs: Any,
    ) -> np.ndarray:
        assert not self._zombie

        # Check whether it's guarded by collector_guard()
        if not self._collector_guarded:
            warnings.warn(
                "Collector is not guarded by FiniteEnv. "
                "This may cause unexpected problems, like unexpected StopIteration exception, "
                "or missing logs.",
                RuntimeWarning,
            )

        # Accept tianshou's `env_id` keyword (and similar variants) for compatibility.
        if env_id is not None:
            id = env_id
        wrapped_id = self._wrap_id(id)
        self._reset_alive_envs()

        # ask super to reset alive envs and remap to current index
        request_id = [i for i in wrapped_id if i in self._alive_env_ids]
        obs = [None] * len(wrapped_id)
        info = [None] * len(wrapped_id)
        id2idx = {i: k for k, i in enumerate(wrapped_id)}
        if request_id:
            # Support different return formats from the underlying VectorEnv.reset:
            # - list of per-env results (each is obs or (obs, info))
            # - a tuple of batched arrays (obs_array, info_array)
            raw = super().reset(request_id)
            results = []
            try:
                # case: returned a tuple of batched arrays, like (obs_array, info_array)
                if isinstance(raw, tuple) and hasattr(raw[0], "__len__") and len(raw[0]) == len(request_id):
                    results = list(zip(*raw))
                # case: returned a list/tuple with one entry per env
                elif isinstance(raw, (list, tuple)) and len(raw) == len(request_id):
                    results = list(raw)
                # case: returned a single ndarray of obs with shape (n, ...)
                elif hasattr(raw, "shape") and raw.shape[0] == len(request_id):
                    results = [raw[i] for i in range(len(request_id))]
                else:
                    results = list(raw)
            except Exception:
                results = list(raw)

            for i, res in zip(request_id, results):
                if isinstance(res, (list, tuple)) and len(res) == 2 and isinstance(res[1], dict):
                    obs_val, info_val = res
                else:
                    obs_val, info_val = res, None
                obs[id2idx[i]] = self._postproc_env_obs(obs_val)
                info[id2idx[i]] = info_val
                # record default info if any
                self._set_default_info(info_val)

        for i, o in zip(wrapped_id, obs):
            if o is None and i in self._alive_env_ids:
                self._alive_env_ids.remove(i)

        # logging
        for i, o in zip(wrapped_id, obs):
            if i in self._alive_env_ids:
                for logger in self._logger:
                    logger.on_env_reset(i, obs)

        # fill empty observation/info with default(fake) observation/info
        for o in obs:
            self._set_default_obs(o)
        for i in info:
            self._set_default_info(i)
        for i, o in enumerate(obs):
            if o is None:
                obs[i] = self._get_default_obs()
        for i, inf in enumerate(info):
            if inf is None:
                info[i] = self._get_default_info()

        if not self._alive_env_ids:
            # comment this line so that the env becomes indispensable
            # self.reset()
            self._zombie = True
            raise StopIteration

        # Try to determine expected observation flattened length from child env's observation_space
        target_len = None
        try:
            envs = getattr(self, "envs", None)
            if envs and len(envs) > 0:
                if hasattr(envs[0], "observation_space"):
                    space = envs[0].observation_space
                elif hasattr(envs[0], "env") and hasattr(envs[0].env, "observation_space"):
                    space = envs[0].env.observation_space
                if hasattr(space, "shape") and space.shape is not None:
                    target_len = int(np.prod(space.shape))
        except Exception:
            target_len = None

        # Fallback: use default_obs if available
        if target_len is None and getattr(self, "_default_obs", None) is not None:
            try:
                tmp = np.asarray(self._default_obs)
                if tmp.dtype != object:
                    target_len = int(tmp.ravel().size)
            except Exception:
                target_len = None

        # Debug: log target_len and incoming obs shapes
        try:
            with open("logs/qlib_finite_env_debug.log", "a") as _f:
                _f.write(f"RESET target_len={target_len} obs_shapes={[getattr(o, 'shape', type(o)) for o in obs]}\n")
        except Exception:
            pass

        obs_arr = _numeric_stack_or_pad(obs, target_len=target_len)
        # Ensure obs array has shape (batch, target_len) when target_len is known.
        try:
            if target_len is not None and obs_arr.dtype != object:
                if obs_arr.ndim == 1:
                    obs_arr = obs_arr.reshape((1, -1))
                # obs_arr now should be 2D: (batch, feat)
                if obs_arr.shape[1] != target_len:
                    # Debug print to surface mismatches during notebook execution
                    try:
                        print(
                            f"FINITE_ENV_DEBUG RESET: target_len={target_len}, obs_shapes={[getattr(o, 'shape', type(o)) for o in obs]}, obs_arr.shape={obs_arr.shape}",
                            flush=True,
                        )
                    except Exception:
                        pass
                    if obs_arr.shape[1] < target_len:
                        pad = np.full((obs_arr.shape[0], target_len - obs_arr.shape[1]), np.nan)
                        obs_arr = np.concatenate([obs_arr, pad], axis=1)
                    else:
                        obs_arr = obs_arr[:, :target_len]
        except Exception:
            # best-effort; fall back to whatever _numeric_stack_or_pad returned
            pass

        info_arr = np.array(info, dtype=object)
        return obs_arr, info_arr

    def step(
        self,
        action: np.ndarray,
        id: int | List[int] | np.ndarray | None = None,
        env_id: int | List[int] | np.ndarray | None = None,
        **kwargs: Any,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        assert not self._zombie
        # Accept tianshou's `env_id` keyword for compatibility
        if env_id is not None:
            id = env_id
        wrapped_id = self._wrap_id(id)
        id2idx = {i: k for k, i in enumerate(wrapped_id)}
        request_id = list(filter(lambda i: i in self._alive_env_ids, wrapped_id))
        # result entries follow Gymnasium API: (obs, rew, terminated, truncated, info)
        result = [[None, None, False, False, None] for _ in range(len(wrapped_id))]

        # ask super to step alive envs and remap to current index
        if request_id:
            valid_act = np.stack([action[id2idx[i]] for i in request_id])
            for i, r in zip(request_id, zip(*super().step(valid_act, request_id))):
                result[id2idx[i]] = list(r)
                result[id2idx[i]][0] = self._postproc_env_obs(result[id2idx[i]][0])

        # logging: adapt to LogWriter.on_env_step(env_id, obs, rew, done, info)
        for i, r in zip(wrapped_id, result):
            if i in self._alive_env_ids:
                obs_i, rew_i, term_i, trunc_i, info_i = r
                for logger in self._logger:
                    logger.on_env_step(i, obs_i, rew_i, term_i, info_i)

        # fill empty observation/info with default(fake)
        for _, r, __, ___, i in result:
            self._set_default_info(i)
            self._set_default_rew(r)
        for i, r in enumerate(result):
            if r[0] is None:
                result[i][0] = self._get_default_obs()
            if r[1] is None:
                result[i][1] = self._get_default_rew()
            if r[4] is None:
                result[i][4] = self._get_default_info()

        obs_r, rew_r, term_r, trunc_r, info_r = map(list, zip(*result))
        # Determine target length similarly for step
        target_len = None
        try:
            envs = getattr(self, "envs", None)
            if envs and len(envs) > 0:
                if hasattr(envs[0], "observation_space"):
                    space = envs[0].observation_space
                elif hasattr(envs[0], "env") and hasattr(envs[0].env, "observation_space"):
                    space = envs[0].env.observation_space
                if hasattr(space, "shape") and space.shape is not None:
                    target_len = int(np.prod(space.shape))
        except Exception:
            target_len = None

        # Fallback: use default_obs if available
        if target_len is None and getattr(self, "_default_obs", None) is not None:
            try:
                tmp = np.asarray(self._default_obs)
                if tmp.dtype != object:
                    target_len = int(tmp.ravel().size)
            except Exception:
                target_len = None

        try:
            with open("logs/qlib_finite_env_debug.log", "a") as _f:
                _f.write(f"STEP target_len={target_len} obs_shapes={[getattr(o, 'shape', type(o)) for o in obs_r]}\n")
        except Exception:
            pass

        ret_obs = _numeric_stack_or_pad(obs_r, target_len=target_len)
        # Enforce feature dimension matches target_len when possible
        try:
            if target_len is not None and ret_obs.dtype != object:
                if ret_obs.ndim == 1:
                    ret_obs = ret_obs.reshape((1, -1))
                if ret_obs.shape[1] != target_len:
                    try:
                        print(
                            f"FINITE_ENV_DEBUG STEP: target_len={target_len}, obs_shapes={[getattr(o, 'shape', type(o)) for o in obs_r]}, ret_obs.shape={ret_obs.shape}",
                            flush=True,
                        )
                    except Exception:
                        pass
                    if ret_obs.shape[1] < target_len:
                        pad = np.full((ret_obs.shape[0], target_len - ret_obs.shape[1]), np.nan)
                        ret_obs = np.concatenate([ret_obs, pad], axis=1)
                    else:
                        ret_obs = ret_obs[:, :target_len]
        except Exception:
            pass
        ret_rew = np.asarray(rew_r)
        ret_term = np.asarray(term_r)
        ret_trunc = np.asarray(trunc_r)
        ret_info = np.array(info_r, dtype=object)
        return cast(Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray], (ret_obs, ret_rew, ret_term, ret_trunc, ret_info))


class FiniteDummyVectorEnv(FiniteVectorEnv, DummyVectorEnv):
    pass


class FiniteSubprocVectorEnv(FiniteVectorEnv, SubprocVectorEnv):
    pass


class FiniteShmemVectorEnv(FiniteVectorEnv, ShmemVectorEnv):
    pass


def vectorize_env(
    env_factory: Callable[..., gym.Env],
    env_type: FiniteEnvType,
    concurrency: int,
    logger: LogWriter | List[LogWriter],
) -> FiniteVectorEnv:
    """Helper function to create a vector env. Can be used to replace usual VectorEnv.

    For example, once you wrote: ::

        DummyVectorEnv([lambda: gym.make(task) for _ in range(env_num)])

    Now you can replace it with: ::

        finite_env_factory(lambda: gym.make(task), "dummy", env_num, my_logger)

    By doing such replacement, you have two additional features enabled (compared to normal VectorEnv):

    1. The vector env will check for NaN observation and kill the worker when its found.
       See :class:`FiniteVectorEnv` for why we need this.
    2. A logger to explicit collect logs from environment workers.

    Parameters
    ----------
    env_factory
        Callable to instantiate one single ``gym.Env``.
        All concurrent workers will have the same ``env_factory``.
    env_type
        dummy or subproc or shmem. Corresponding to
        `parallelism in tianshou <https://tianshou.readthedocs.io/en/master/api/tianshou.env.html#vectorenv>`_.
    concurrency
        Concurrent environment workers.
    logger
        Log writers.

    Warnings
    --------
    Please do not use lambda expression here for ``env_factory`` as it may create incorrectly-shared instances.

    Don't do: ::

        vectorize_env(lambda: EnvWrapper(...), ...)

    Please do: ::

        def env_factory(): ...
        vectorize_env(env_factory, ...)
    """
    env_type_cls_mapping: Dict[str, Type[FiniteVectorEnv]] = {
        "dummy": FiniteDummyVectorEnv,
        "subproc": FiniteSubprocVectorEnv,
        "shmem": FiniteShmemVectorEnv,
    }

    finite_env_cls = env_type_cls_mapping[env_type]

    return finite_env_cls(logger, [env_factory for _ in range(concurrency)])
